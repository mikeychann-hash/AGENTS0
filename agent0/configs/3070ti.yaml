# ============================================
# LOCAL DEVELOPMENT CONFIGURATION
# ============================================
# WARNING: No sandboxing or isolation!
# Code runs directly on your machine.
# Only use with trusted tasks.
# ============================================

# 3070 Ti-friendly defaults (8 GB VRAM target).
models:
  teacher:
    backend: ollama
    model: qwen2.5:3b
    host: http://127.0.0.1:11434
    context_length: 4096
    temperature: 0.7
    top_p: 0.9
    uncertainty_samples: 3
  student:
    backend: ollama
    model: qwen2.5:7b
    host: http://127.0.0.1:11434
    context_length: 8192
    temperature: 0.6
    top_p: 0.9
    uncertainty_samples: 3

resources:
  device: cuda
  max_gpu_memory_gb: 8
  num_threads: 6
  max_tokens_per_task: 512

tooling:
  # LOCAL MODE: Limited tools for safety
  enable_python: true
  enable_shell: false
  enable_math: true
  enable_tests: false
  timeout_seconds: 30
  workdir: ./sandbox
  allowed_shell: []

rewards:
  weight_uncertainty: 0.5
  weight_tool_use: 0.3
  weight_novelty: 0.2
  target_success_rate: 0.5
  repetition_similarity_threshold: 0.9

logging:
  base_dir: ./runs
  save_every: 10
  flush_every: 1

router:
  enable: true
  cloud_confidence_threshold: 0.7
  local_confidence_threshold: 0.4
  cache_path: ./runs/router_cache.json
  cloud_command: ""  # e.g., openai api chat.completions.create -m gpt-4o-mini

embedding:
  use_transformer: true
  model_name: all-MiniLM-L6-v2
